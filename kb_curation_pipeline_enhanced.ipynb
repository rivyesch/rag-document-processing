{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c2142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
    "# from ragas.metrics.critique import harmfulness\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from llama_index.core import Document\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87cbdccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llama_cloud_api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "os.environ['LLAMA_CLOUD_API_KEY'] = llama_cloud_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a86ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Setting up LlamaIndex...\n",
      "‚úÖ LlamaIndex setup complete\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 1: Setting up LlamaIndex...\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4.1-mini\",  # or \"gpt-3.5-turbo\" for cost efficiency\n",
    "    temperature=0.1,\n",
    "    api_key=openai_api_key  # Make sure openai_api_key is defined\n",
    ")\n",
    "\n",
    "# Initialize settings\n",
    "Settings.llm = OpenAI(\n",
    "    model=\"gpt-4.1-mini\", \n",
    "    temperature=0.1, \n",
    "    api_key=openai_api_key\n",
    "    )\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "    \n",
    "# Configure chunking settings\n",
    "# Settings.chunk_size = 1024\n",
    "# Settings.chunk_overlap = 200\n",
    "\n",
    "print(\"‚úÖ LlamaIndex setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e2ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 (Baseline): Parsing document without LLM...\n",
      "Found cached baseline markdown at: ./parsed_docs/apple_2021_10k_baseline.md\n",
      "‚úÖ Baseline parsed. Nodes: 77 | Markdown saved: ./parsed_docs/apple_2021_10k_baseline.md\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Document Parsing - Choose Your Method\n",
    "print(\"Step 2: Document Parsing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def parse_document(method=\"baseline\"):\n",
    "    \"\"\"\n",
    "    Parse document using the specified method.\n",
    "    \n",
    "    Args:\n",
    "        method (str): 'baseline', 'llamaparse', or 'docling'\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (markdown_content, markdown_nodes, method_used)\n",
    "    \"\"\"\n",
    "    from llama_index.core import Document\n",
    "    from llama_index.core.node_parser import MarkdownNodeParser\n",
    "    from llama_index.core import SimpleDirectoryReader\n",
    "    from llama_index.core.node_parser import SentenceSplitter\n",
    "    \n",
    "    file_path = \"./data/apple_2021_10k.pdf\"\n",
    "    cache_dir = \"./parsed_docs\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    file_stem = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    if method == \"baseline\":\n",
    "        print(\"üîß Using Baseline parsing (Simple text extraction)...\")\n",
    "        output_md = os.path.join(cache_dir, f\"{file_stem}_baseline.md\")\n",
    "        \n",
    "        if os.path.exists(output_md):\n",
    "            print(f\"üìÅ Found cached baseline markdown at: {output_md}\")\n",
    "            with open(output_md, \"r\", encoding=\"utf-8\") as f:\n",
    "                markdown_content = f.read()\n",
    "            \n",
    "            document = Document(text=markdown_content, metadata={\"source\": file_path})\n",
    "            splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "            markdown_nodes = splitter.get_nodes_from_documents([document])\n",
    "        else:\n",
    "            print(\"üîÑ Parsing document with baseline method...\")\n",
    "            docs = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "            raw_text = \"\\n\\n\".join([d.text or \"\" for d in docs])\n",
    "            markdown_content = raw_text\n",
    "            \n",
    "            with open(output_md, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            document = Document(text=markdown_content, metadata={\"source\": file_path})\n",
    "            splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "            markdown_nodes = splitter.get_nodes_from_documents([document])\n",
    "        \n",
    "        print(f\"‚úÖ Baseline parsing complete. Nodes: {len(markdown_nodes)}\")\n",
    "        \n",
    "    elif method == \"llamaparse\":\n",
    "        print(\"ü§ñ Using LlamaParse (AI-powered parsing)...\")\n",
    "        output_md = os.path.join(cache_dir, f\"{file_stem}_llama.md\")\n",
    "        \n",
    "        if os.path.exists(output_md):\n",
    "            print(f\"üìÅ Found cached LlamaParse markdown at: {output_md}\")\n",
    "            with open(output_md, 'r', encoding='utf-8') as f:\n",
    "                markdown_content = f.read()\n",
    "            \n",
    "            document = Document(text=markdown_content, metadata={\"source\": file_path})\n",
    "            markdown_parser = MarkdownNodeParser()\n",
    "            markdown_nodes = markdown_parser.get_nodes_from_documents([document])\n",
    "        else:\n",
    "            print(\"üîÑ Parsing document with LlamaParse...\")\n",
    "            parser = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                parse_mode=\"parse_page_with_llm\",\n",
    "                high_res_ocr=True,\n",
    "                adaptive_long_table=True,\n",
    "                outlined_table_extraction=True,\n",
    "                output_tables_as_HTML=True,\n",
    "            )\n",
    "            \n",
    "            # Use synchronous parsing for LlamaParse\n",
    "            result = parser.parse(file_path)\n",
    "            markdown_nodes = result.get_markdown_nodes(split_by_page=True)\n",
    "            markdown_content = \"\\n\\n\".join([node.text for node in markdown_nodes])\n",
    "            \n",
    "            with open(output_md, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "        \n",
    "        print(f\"‚úÖ LlamaParse complete. Nodes: {len(markdown_nodes)}\")\n",
    "        \n",
    "    elif method == \"docling\":\n",
    "        print(\"üìÑ Using Docling (Advanced document parsing)...\")\n",
    "        output_md = os.path.join(cache_dir, f\"{file_stem}_docling.md\")\n",
    "        \n",
    "        if os.path.exists(output_md):\n",
    "            print(f\"üìÅ Found cached Docling markdown at: {output_md}\")\n",
    "            with open(output_md, \"r\", encoding=\"utf-8\") as f:\n",
    "                markdown_content = f.read()\n",
    "        else:\n",
    "            print(\"üîÑ Parsing document with Docling...\")\n",
    "            try:\n",
    "                from docling.document_converter import DocumentConverter, DocumentConversionInput\n",
    "                converter = DocumentConverter()\n",
    "                result = converter.convert(DocumentConversionInput.from_path(file_path))\n",
    "            except ImportError:\n",
    "                from docling.document_converter import DocumentConverter\n",
    "                converter = DocumentConverter()\n",
    "                result = converter.convert(file_path)\n",
    "            \n",
    "            if hasattr(result.document, \"export_to_markdown\"):\n",
    "                markdown_content = result.document.export_to_markdown()\n",
    "            else:\n",
    "                markdown_content = result.document.export_to_text()\n",
    "            \n",
    "            with open(output_md, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(markdown_content)\n",
    "        \n",
    "        document = Document(text=markdown_content, metadata={\"source\": file_path})\n",
    "        markdown_nodes = MarkdownNodeParser().get_nodes_from_documents([document])\n",
    "        print(f\"‚úÖ Docling complete. Nodes: {len(markdown_nodes)}\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Choose from: 'baseline', 'llamaparse', 'docling'\")\n",
    "    \n",
    "    return markdown_content, markdown_nodes, method\n",
    "\n",
    "# Interactive method selection\n",
    "print(\"\\nüìã Available parsing methods:\")\n",
    "print(\"1. baseline  - Simple text extraction (fastest)\")\n",
    "print(\"2. llamaparse - AI-powered parsing (most accurate)\")\n",
    "print(\"3. docling   - Advanced document parsing (balanced)\")\n",
    "\n",
    "# Get user input\n",
    "method_choice = input(\"\\nüéØ Choose parsing method (1, 2, or 3): \").strip()\n",
    "\n",
    "method_map = {\n",
    "    \"1\": \"baseline\",\n",
    "    \"2\": \"llamaparse\", \n",
    "    \"3\": \"docling\"\n",
    "}\n",
    "\n",
    "if method_choice not in method_map:\n",
    "    print(\"‚ö†Ô∏è  Invalid choice. Defaulting to baseline method.\")\n",
    "    method_choice = \"1\"\n",
    "\n",
    "selected_method = method_map[method_choice]\n",
    "print(f\"\\nüöÄ Selected method: {selected_method.upper()}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Parse document with selected method\n",
    "markdown_content, markdown_nodes, method_used = parse_document(selected_method)\n",
    "\n",
    "# Store the selected method globally for use in evaluation saving\n",
    "PARSING_METHOD = method_used\n",
    "\n",
    "print(f\"\\nüéâ Document parsing complete!\")\n",
    "print(f\"üìä Method used: {method_used.upper()}\")\n",
    "print(f\"üìÑ Content length: {len(markdown_content):,} characters\")\n",
    "print(f\"üîó Number of nodes: {len(markdown_nodes)}\")\n",
    "print(f\"üíæ Cached for future use\")\n",
    "print(f\"üè∑Ô∏è  Method '{method_used}' will be used for evaluation file naming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e7d4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fafa00fdcf49c8b4923369e571836d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 64951 tokens (64951 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorIndexRetriever\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquery_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrieverQueryEngine\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m markdown_index = \u001b[43mVectorStoreIndex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmarkdown_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Configure retriever with better settings\u001b[39;00m\n\u001b[32m     17\u001b[39m retriever = VectorIndexRetriever(\n\u001b[32m     18\u001b[39m     index=markdown_index,\n\u001b[32m     19\u001b[39m     similarity_top_k=\u001b[32m5\u001b[39m,  \u001b[38;5;66;03m# Retrieve more candidates\u001b[39;00m\n\u001b[32m     20\u001b[39m     vector_store_query_mode=\u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Combines semantic and keyword search\u001b[39;00m\n\u001b[32m     21\u001b[39m     alpha=\u001b[32m0.3\u001b[39m  \u001b[38;5;66;03m# Weight between semantic (0) and keyword (1) search\u001b[39;00m\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:75\u001b[39m, in \u001b[36mVectorStoreIndex.__init__\u001b[39m\u001b[34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mself\u001b[39m._embed_model = resolve_embed_model(\n\u001b[32m     71\u001b[39m     embed_model \u001b[38;5;129;01mor\u001b[39;00m Settings.embed_model, callback_manager=callback_manager\n\u001b[32m     72\u001b[39m )\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m._insert_batch_size = insert_batch_size\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/llama_index/core/indices/base.py:79\u001b[39m, in \u001b[36mBaseIndex.__init__\u001b[39m\u001b[34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m     nodes = nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     index_struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m._index_struct = index_struct\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m._storage_context.index_store.add_index_struct(\u001b[38;5;28mself\u001b[39m._index_struct)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:309\u001b[39m, in \u001b[36mVectorStoreIndex.build_index_from_nodes\u001b[39m\u001b[34m(self, nodes, **insert_kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(content_nodes) != \u001b[38;5;28mlen\u001b[39m(nodes):\n\u001b[32m    307\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSome nodes are missing content, skipping them...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:278\u001b[39m, in \u001b[36mVectorStoreIndex._build_index_from_nodes\u001b[39m\u001b[34m(self, nodes, **insert_kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m     run_async_tasks(tasks)\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:231\u001b[39m, in \u001b[36mVectorStoreIndex._add_nodes_to_index\u001b[39m\u001b[34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m._insert_batch_size):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     nodes_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_node_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m     new_ids = \u001b[38;5;28mself\u001b[39m._vector_store.add(nodes_batch, **insert_kwargs)\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._vector_store.stores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._store_nodes_override:\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[32m    236\u001b[39m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:138\u001b[39m, in \u001b[36mVectorStoreIndex._get_node_with_embedding\u001b[39m\u001b[34m(self, nodes, show_progress)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_node_with_embedding\u001b[39m(\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    128\u001b[39m     nodes: Sequence[BaseNode],\n\u001b[32m    129\u001b[39m     show_progress: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    130\u001b[39m ) -> List[BaseNode]:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[32m    133\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m \n\u001b[32m    137\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     id_to_embed_map = \u001b[43membed_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     results = []\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/llama_index/core/indices/utils.py:176\u001b[39m, in \u001b[36membed_nodes\u001b[39m\u001b[34m(nodes, embed_model, show_progress)\u001b[39m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    174\u001b[39m         id_to_embed_map[node.node_id] = node.embedding\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m new_embeddings = \u001b[43membed_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_text_embedding_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts_to_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[32m    181\u001b[39m     id_to_embed_map[new_id] = text_embedding\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/llama_index_instrumentation/dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:473\u001b[39m, in \u001b[36mBaseEmbedding.get_text_embedding_batch\u001b[39m\u001b[34m(self, texts, show_progress, **kwargs)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    469\u001b[39m     CBEventType.EMBEDDING,\n\u001b[32m    470\u001b[39m     payload={EventPayload.SERIALIZED: \u001b[38;5;28mself\u001b[39m.to_dict()},\n\u001b[32m    471\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[32m    472\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embeddings_cache:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m         embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embeddings_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    475\u001b[39m         embeddings = \u001b[38;5;28mself\u001b[39m._get_text_embeddings_cached(cur_batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:472\u001b[39m, in \u001b[36mOpenAIEmbedding._get_text_embeddings\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_retryable_get_embeddings\u001b[39m():\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m get_embeddings(\n\u001b[32m    466\u001b[39m         client,\n\u001b[32m    467\u001b[39m         texts,\n\u001b[32m    468\u001b[39m         engine=\u001b[38;5;28mself\u001b[39m._text_engine,\n\u001b[32m    469\u001b[39m         **\u001b[38;5;28mself\u001b[39m.additional_kwargs,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_retryable_get_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:465\u001b[39m, in \u001b[36mOpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_retryable_get_embeddings\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_text_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:172\u001b[39m, in \u001b[36mget_embeddings\u001b[39m\u001b[34m(client, list_of_text, engine, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_text) <= \u001b[32m2048\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mThe batch size should not be larger than 2048.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m list_of_text = [text.replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m list_of_text]\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m data = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mlist_of_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.data\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [d.embedding \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/openai/resources/embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/tutorials/RAG/.venv/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 64951 tokens (64951 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "# Setup our LlamaParse Indexes\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "markdown_index = VectorStoreIndex(\n",
    "    nodes=markdown_nodes, \n",
    "    show_progress=True\n",
    "    )\n",
    "\n",
    "# Configure retriever with better settings\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=markdown_index,\n",
    "    similarity_top_k=5,  # Retrieve more candidates\n",
    "    vector_store_query_mode=\"default\",  # Combines semantic and keyword search\n",
    "    alpha=0.3  # Weight between semantic (0) and keyword (1) search\n",
    ")\n",
    "\n",
    "markdown_query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.6)  # Filter out low similarity results\n",
    "    ],\n",
    "    response_mode=\"compact\"  # Better for multi-document responses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a5f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********Markdown Query Engine***********\n",
      "Apple incorporates the 2022 Proxy Statement by reference in Part III of the Annual Report on Form 10-K, specifically for the information required in Items 10, 11, 12, and 13.\n"
     ]
    }
   ],
   "source": [
    "query_1 = \"Where in the 10-K does Apple incorporate the 2022 Proxy Statement by reference?\"\n",
    "\n",
    "response_1 = await markdown_query_engine.aquery(query_1)\n",
    "print(\"\\n***********Markdown Query Engine***********\")\n",
    "print(response_1)\n",
    "\n",
    "# for node in response_1.source_nodes:\n",
    "#     print(\"\\n***********Source Node***********\")\n",
    "#     print(node.node.text)      # chunk text\n",
    "#     print(node.node.metadata)  # chunk metadata\n",
    "#     print(node.score)          # similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a14e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 35 QA pairs\n"
     ]
    }
   ],
   "source": [
    "# === Load QA pairs (question, reference) ===\n",
    "# Expects a JSON list of objects: {\"question\": str, \"reference\": str}\n",
    "# If qa_pairs.json is missing, falls back to a small built-in sample.\n",
    "\n",
    "import os, json\n",
    "from typing import List, Dict\n",
    "\n",
    "qa_path = \"/Users/rivyesch/Dev/tutorials/RAG/qa_pairs.json\"\n",
    "\n",
    "def load_qa_pairs(path: str) -> List[Dict[str, str]]:\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        # basic validation\n",
    "        cleaned = []\n",
    "        for i, item in enumerate(data):\n",
    "            q = item.get(\"question\")\n",
    "            r = item.get(\"reference\")\n",
    "            if isinstance(q, str) and isinstance(r, str):\n",
    "                cleaned.append({\"question\": q.strip(), \"reference\": r.strip()})\n",
    "        if not cleaned:\n",
    "            raise ValueError(\"qa_pairs.json is present but empty or invalid. Expected list of {question, reference}.\")\n",
    "        return cleaned\n",
    "    # Fallback sample (replace by providing qa_pairs.json)\n",
    "    print(\"qa_pairs.json not found. Using a small built-in sample.\")\n",
    "    return [\n",
    "        {\"question\": \"What is Apple‚Äôs fiscal year-end date in the 2021 Form 10-K?\", \"reference\": \"September 25, 2021.\"},\n",
    "        {\"question\": \"How many shares of common stock were outstanding as of October 15, 2021?\", \"reference\": \"16,406,397,000 shares.\"},\n",
    "        {\"question\": \"Which iPhone models were released in October and November 2020?\", \"reference\": \"iPhone 12, iPhone 12 mini, iPhone 12 Pro, and iPhone 12 Pro Max.\"},\n",
    "    ]\n",
    "\n",
    "qa_pairs = load_qa_pairs(qa_path)\n",
    "print(f\"Loaded {len(qa_pairs)} QA pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfd4fea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference‚Üínode mapping: matched=35, unmatched=0 of 35\n",
      "Evaluating 35 QA pairs with LlamaIndex metrics...\n",
      "#1 HR=0.00 MRR=0.00 nDCG=0.00\n",
      "#2 HR=1.00 MRR=1.00 nDCG=0.91\n",
      "#3 HR=1.00 MRR=1.00 nDCG=1.00\n",
      "#4 HR=1.00 MRR=1.00 nDCG=1.00\n",
      "#5 HR=1.00 MRR=1.00 nDCG=1.00\n",
      "#6 HR=1.00 MRR=0.50 nDCG=0.30\n",
      "#7 HR=0.00 MRR=0.00 nDCG=0.00\n",
      "#8 HR=1.00 MRR=0.50 nDCG=0.30\n",
      "#9 HR=0.00 MRR=0.00 nDCG=0.00\n",
      "#10 HR=1.00 MRR=0.50 nDCG=0.73\n",
      "#11 HR=1.00 MRR=0.50 nDCG=0.48\n",
      "#12 HR=1.00 MRR=1.00 nDCG=1.00\n",
      "#13 HR=1.00 MRR=1.00 nDCG=0.77\n",
      "#14 HR=1.00 MRR=1.00 nDCG=0.91\n",
      "#15 HR=1.00 MRR=1.00 nDCG=1.00\n",
      "#16 HR=1.00 MRR=1.00 nDCG=1.00\n",
      "#17 HR=1.00 MRR=1.00 nDCG=0.47\n",
      "#18 HR=1.00 MRR=1.00 nDCG=0.47\n",
      "#19 HR=1.00 MRR=0.25 nDCG=0.20\n",
      "#20 HR=1.00 MRR=1.00 nDCG=0.67\n",
      "#21 HR=1.00 MRR=1.00 nDCG=0.77\n",
      "#22 HR=1.00 MRR=0.50 nDCG=0.30\n",
      "#23 HR=1.00 MRR=0.50 nDCG=0.53\n",
      "#24 HR=1.00 MRR=1.00 nDCG=0.47\n",
      "#25 HR=1.00 MRR=0.50 nDCG=0.30\n",
      "#26 HR=1.00 MRR=0.25 nDCG=0.20\n",
      "#27 HR=1.00 MRR=1.00 nDCG=0.70\n",
      "#28 HR=1.00 MRR=1.00 nDCG=0.70\n",
      "#29 HR=1.00 MRR=0.50 nDCG=0.48\n",
      "#30 HR=1.00 MRR=0.50 nDCG=0.53\n",
      "#31 HR=1.00 MRR=1.00 nDCG=0.70\n",
      "#32 HR=1.00 MRR=1.00 nDCG=0.89\n",
      "#33 HR=1.00 MRR=0.50 nDCG=0.63\n",
      "#34 HR=1.00 MRR=0.50 nDCG=0.53\n",
      "#35 HR=0.00 MRR=0.00 nDCG=0.00\n",
      "\n",
      "LlamaIndex averages over 35 evaluated QA pairs:\n",
      "- Hit Rate: 0.886\n",
      "- MRR:      0.686\n",
      "- nDCG:     0.569\n"
     ]
    }
   ],
   "source": [
    "# === LlamaIndex retrieval evaluation using QA references (robust mapping) ===\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "    _HAS_RAPIDFUZZ = True\n",
    "except Exception:\n",
    "    _HAS_RAPIDFUZZ = False\n",
    "\n",
    "# Normalize to reduce false negatives (quotes, dashes, punctuation, whitespace)\n",
    "_def_dash = re.compile(r\"[\\u2012\\u2013\\u2014\\u2015]\")\n",
    "_def_quotes = str.maketrans({\"‚Äú\": '\"', \"‚Äù\": '\"', \"‚Äò\": \"'\", \"‚Äô\": \"'\"})\n",
    "_whitespace = re.compile(r\"\\s+\")\n",
    "_punct = re.compile(r\"[^\\w\\s.$%+-]\")\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text.translate(_def_quotes)\n",
    "    t = _def_dash.sub(\"-\", t)\n",
    "    t = t.lower().strip()\n",
    "    t = _punct.sub(\" \", t)\n",
    "    t = _whitespace.sub(\" \", t)\n",
    "    return t\n",
    "\n",
    "def map_reference_to_node_ids(reference: str, nodes: List, top_k_fallback: int = 3) -> List[str]:\n",
    "    # 1) normalized substring\n",
    "    ref_norm = normalize_text(reference)\n",
    "    matched: List[str] = []\n",
    "    if ref_norm:\n",
    "        for n in nodes:\n",
    "            try:\n",
    "                if ref_norm in normalize_text(n.text or \"\"):\n",
    "                    matched.append(n.node_id)\n",
    "            except Exception:\n",
    "                continue\n",
    "        if matched:\n",
    "            return list(dict.fromkeys(matched))\n",
    "\n",
    "    # 2) fuzzy partial match\n",
    "    if _HAS_RAPIDFUZZ and ref_norm:\n",
    "        scored = []\n",
    "        for n in nodes:\n",
    "            try:\n",
    "                s = fuzz.partial_ratio(ref_norm, normalize_text(n.text or \"\"))\n",
    "                scored.append((s, n.node_id))\n",
    "            except Exception:\n",
    "                continue\n",
    "        scored.sort(reverse=True)\n",
    "        fuzzy_ids = [nid for s, nid in scored[:top_k_fallback] if s >= 70]\n",
    "        if fuzzy_ids:\n",
    "            return list(dict.fromkeys(fuzzy_ids))\n",
    "\n",
    "    # 3) vector similarity fallback using retriever\n",
    "    try:\n",
    "        cands = retriever.retrieve(reference)\n",
    "        vs_ids = [c.node.node_id for c in (cands or [])][:top_k_fallback]\n",
    "        if vs_ids:\n",
    "            return list(dict.fromkeys(vs_ids))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return []\n",
    "\n",
    "# Build expected ids and report coverage\n",
    "qa_expected_ids: List[List[str]] = []\n",
    "unmatched = 0\n",
    "for item in qa_pairs:\n",
    "    ids = map_reference_to_node_ids(item[\"reference\"], markdown_nodes)\n",
    "    if not ids:\n",
    "        unmatched += 1\n",
    "    qa_expected_ids.append(ids)\n",
    "print(f\"Reference‚Üínode mapping: matched={len(qa_pairs)-unmatched}, unmatched={unmatched} of {len(qa_pairs)}\")\n",
    "\n",
    "# Evaluate retrieval\n",
    "metrics = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(metrics, retriever=retriever)\n",
    "\n",
    "li_results: List[Dict] = []\n",
    "print(f\"Evaluating {len(qa_pairs)} QA pairs with LlamaIndex metrics...\")\n",
    "for i, item in enumerate(qa_pairs, start=1):\n",
    "    expected_ids = qa_expected_ids[i-1]\n",
    "    if not expected_ids:\n",
    "        print(f\"‚ö†Ô∏è  QA {i} has no matched expected_ids. Skipping.\")\n",
    "        continue\n",
    "    res = retriever_evaluator.evaluate(query=item[\"question\"], expected_ids=expected_ids)\n",
    "    li_results.append({\n",
    "        \"index\": i,\n",
    "        \"question\": item[\"question\"],\n",
    "        \"reference\": item[\"reference\"],\n",
    "        \"expected_ids\": expected_ids,\n",
    "        \"retrieved_ids\": res.retrieved_ids,\n",
    "        \"metrics\": res.metric_vals_dict,\n",
    "    })\n",
    "    m = res.metric_vals_dict\n",
    "    print(f\"#{i} HR={m.get('hit_rate',0):.2f} MRR={m.get('mrr',0):.2f} nDCG={m.get('ndcg',0):.2f}\")\n",
    "\n",
    "if li_results:\n",
    "    n = len(li_results)\n",
    "    avg_hr = sum(r[\"metrics\"].get(\"hit_rate\", 0.0) for r in li_results) / n\n",
    "    avg_mrr = sum(r[\"metrics\"].get(\"mrr\", 0.0) for r in li_results) / n\n",
    "    avg_ndcg = sum(r[\"metrics\"].get(\"ndcg\", 0.0) for r in li_results) / n\n",
    "    print(f\"\\nLlamaIndex averages over {n} evaluated QA pairs:\")\n",
    "    print(f\"- Hit Rate: {avg_hr:.3f}\")\n",
    "    print(f\"- MRR:      {avg_mrr:.3f}\")\n",
    "    print(f\"- nDCG:     {avg_ndcg:.3f}\")\n",
    "else:\n",
    "    print(\"No LlamaIndex results to aggregate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06dade51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd8a81812e74711b2a5dc665e26e821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382994bd2ba641af82f493498efdf88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1/9:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS:\n",
      "- context_precision: 0.7554\n",
      "- context_recall: 0.8286\n",
      "- faithfulness: 0.6888\n",
      "- answer_relevancy: 0.7022\n"
     ]
    }
   ],
   "source": [
    "# === RAGAS evaluation (robust, safe printing) ===\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_recall, context_precision\n",
    "\n",
    "questions = [q[\"question\"] for q in qa_pairs]\n",
    "references = [q[\"reference\"] for q in qa_pairs]\n",
    "\n",
    "answers = []\n",
    "all_contexts = []\n",
    "for q in questions:\n",
    "    resp = markdown_query_engine.query(q)\n",
    "    answers.append(str(resp))\n",
    "    ctx = [sn.node.text for sn in (resp.source_nodes or [])]\n",
    "    # Ensure non-empty contexts; if empty, fall back to retriever top-k to avoid downstream errors\n",
    "    if not ctx:\n",
    "        try:\n",
    "            cands = retriever.retrieve(q)\n",
    "            ctx = [c.node.text for c in (cands or [])]\n",
    "        except Exception:\n",
    "            ctx = []\n",
    "    all_contexts.append(ctx)\n",
    "\n",
    "ragas_ds = Dataset.from_dict({\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": all_contexts,\n",
    "    \"reference\": references,  # can also be a list[str] per row if you have multiple gold references\n",
    "})\n",
    "\n",
    "# Smaller batch_size can avoid worker IndexErrors; disable progress for cleaner logs if desired\n",
    "ragas_result = evaluate(\n",
    "    dataset=ragas_ds,\n",
    "    metrics=[context_precision, context_recall, faithfulness, answer_relevancy],\n",
    "    batch_size=16,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "print(\"RAGAS:\")\n",
    "try:\n",
    "    df = ragas_result.to_pandas()\n",
    "    cols = [c for c in [\"context_precision\",\"context_recall\",\"faithfulness\",\"answer_relevancy\"] if c in df.columns]\n",
    "    agg = {c: float(df[c].mean()) for c in cols}\n",
    "    for k, v in agg.items():\n",
    "        print(f\"- {k}: {v:.4f}\")\n",
    "except Exception as e:\n",
    "    # Fallback printing if API differs\n",
    "    print(\"Could not summarize via DataFrame:\", e)\n",
    "    print(ragas_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4d76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LlamaIndex results -> /Users/rivyesch/Dev/tutorials/RAG/llamaindex_metrics_20250922_235056.json\n",
      "Saved LlamaIndex aggregates -> /Users/rivyesch/Dev/tutorials/RAG/llamaindex_aggregate_metrics_20250922_235056.json\n",
      "Saved RAGAS aggregates -> /Users/rivyesch/Dev/tutorials/RAG/ragas_aggregate_metrics_20250922_235056.json\n",
      "Saved RAGAS per-row metrics -> /Users/rivyesch/Dev/tutorials/RAG/ragas_metrics_20250922_235056.csv\n"
     ]
    }
   ],
   "source": [
    "# === Save evaluation artifacts organized by parsing method ===\n",
    "import os, json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create organized directory structure\n",
    "base_dir = \"./evaluation\"\n",
    "method_dir = os.path.join(base_dir, PARSING_METHOD.title())  # e.g., \"./evaluation/Baseline\", \"./evaluation/Llamaparse\", \"./evaluation/Docling\"\n",
    "os.makedirs(method_dir, exist_ok=True)\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"üìÅ Saving evaluation results to: {method_dir}\")\n",
    "print(f\"üè∑Ô∏è  Parsing method: {PARSING_METHOD.upper()}\")\n",
    "print(f\"‚è∞ Timestamp: {ts}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save LlamaIndex results (per-QA JSON + aggregates JSON)\n",
    "try:\n",
    "    li_full_path = os.path.join(method_dir, f\"llamaindex_metrics_{PARSING_METHOD}_{ts}.json\")\n",
    "    with open(li_full_path, \"w\") as f:\n",
    "        json.dump(li_results, f, indent=2)\n",
    "\n",
    "    # compute aggregates from li_results\n",
    "    def _avg(key: str) -> float:\n",
    "        vals = [r.get(\"metrics\", {}).get(key) for r in li_results]\n",
    "        vals = [v for v in vals if isinstance(v, (int, float))]\n",
    "        return float(sum(vals) / len(vals)) if vals else 0.0\n",
    "\n",
    "    li_aggregates = {\n",
    "        \"parsing_method\": PARSING_METHOD,\n",
    "        \"hit_rate\": _avg(\"hit_rate\"),\n",
    "        \"mrr\": _avg(\"mrr\"),\n",
    "        \"ndcg\": _avg(\"ndcg\"),\n",
    "        \"precision\": _avg(\"precision\"),\n",
    "        \"recall\": _avg(\"recall\"),\n",
    "        \"ap\": _avg(\"ap\"),\n",
    "        \"evaluated_pairs\": len(li_results),\n",
    "        \"timestamp\": ts\n",
    "    }\n",
    "\n",
    "    li_agg_path = os.path.join(method_dir, f\"llamaindex_aggregate_metrics_{PARSING_METHOD}_{ts}.json\")\n",
    "    with open(li_agg_path, \"w\") as f:\n",
    "        json.dump({\"aggregates\": li_aggregates}, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ LlamaIndex results -> {li_full_path}\")\n",
    "    print(f\"‚úÖ LlamaIndex aggregates -> {li_agg_path}\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Could not save LlamaIndex artifacts:\", e)\n",
    "\n",
    "# Save RAGAS metrics (aggregates + per-row CSV)\n",
    "try:\n",
    "    ragas_json_path = os.path.join(method_dir, f\"ragas_aggregate_metrics_{PARSING_METHOD}_{ts}.json\")\n",
    "    ragas_csv_path = os.path.join(method_dir, f\"ragas_metrics_{PARSING_METHOD}_{ts}.csv\")\n",
    "\n",
    "    # Prefer DataFrame output from EvaluationResult\n",
    "    df = ragas_result.to_pandas()\n",
    "    cols = [c for c in [\"context_precision\",\"context_recall\",\"faithfulness\",\"answer_relevancy\"] if c in df.columns]\n",
    "    aggregates = {\n",
    "        \"parsing_method\": PARSING_METHOD,\n",
    "        \"timestamp\": ts\n",
    "    }\n",
    "    aggregates.update({c: float(df[c].mean()) for c in cols})\n",
    "\n",
    "    # Save aggregates JSON\n",
    "    with open(ragas_json_path, \"w\") as f:\n",
    "        json.dump({\"aggregates\": aggregates}, f, indent=2)\n",
    "\n",
    "    # Save full per-row metrics CSV\n",
    "    try:\n",
    "        df.to_csv(ragas_csv_path, index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"‚úÖ RAGAS aggregates -> {ragas_json_path}\")\n",
    "    print(f\"‚úÖ RAGAS per-row metrics -> {ragas_csv_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Fallback: try to_dict; else repr\n",
    "    try:\n",
    "        ragas_fallback_path = os.path.join(method_dir, f\"ragas_metrics_{PARSING_METHOD}_{ts}.json\")\n",
    "        payload = getattr(ragas_result, \"to_dict\", lambda: {\"result\": repr(ragas_result)})()\n",
    "        with open(ragas_fallback_path, \"w\") as f:\n",
    "            json.dump(payload, f, indent=2, default=str)\n",
    "        print(f\"‚úÖ RAGAS fallback -> {ragas_fallback_path}\")\n",
    "    except Exception as e2:\n",
    "        print(\"‚ùå Could not save RAGAS metrics:\", e2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"üéâ All evaluation results saved for {PARSING_METHOD.upper()} parsing method!\")\n",
    "print(f\"üìÅ Location: {method_dir}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optional: Compare results across parsing methods ===\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def compare_parsing_methods():\n",
    "    \"\"\"\n",
    "    Compare evaluation results across different parsing methods.\n",
    "    This function reads all saved evaluation results and creates a comparison.\n",
    "    \"\"\"\n",
    "    base_dir = Path(\"./evaluation\")\n",
    "    \n",
    "    if not base_dir.exists():\n",
    "        print(\"‚ùå No evaluation directory found. Run evaluations first.\")\n",
    "        return\n",
    "    \n",
    "    comparison_data = {}\n",
    "    \n",
    "    # Find all method directories\n",
    "    for method_dir in base_dir.iterdir():\n",
    "        if method_dir.is_dir():\n",
    "            method_name = method_dir.name\n",
    "            print(f\"\\nüîç Analyzing {method_name} results...\")\n",
    "            \n",
    "            # Find the most recent aggregate files\n",
    "            llama_files = list(method_dir.glob(f\"llamaindex_aggregate_metrics_{method_name.lower()}*.json\"))\n",
    "            ragas_files = list(method_dir.glob(f\"ragas_aggregate_metrics_{method_name.lower()}*.json\"))\n",
    "            \n",
    "            method_data = {\"method\": method_name}\n",
    "            \n",
    "            # Load LlamaIndex results\n",
    "            if llama_files:\n",
    "                latest_llama = max(llama_files, key=lambda x: x.stat().st_mtime)\n",
    "                try:\n",
    "                    with open(latest_llama, 'r') as f:\n",
    "                        llama_data = json.load(f)\n",
    "                        method_data[\"llamaindex\"] = llama_data.get(\"aggregates\", {})\n",
    "                        print(f\"  ‚úÖ LlamaIndex: HR={method_data['llamaindex'].get('hit_rate', 0):.3f}, MRR={method_data['llamaindex'].get('mrr', 0):.3f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå Error reading LlamaIndex results: {e}\")\n",
    "            \n",
    "            # Load RAGAS results\n",
    "            if ragas_files:\n",
    "                latest_ragas = max(ragas_files, key=lambda x: x.stat().st_mtime)\n",
    "                try:\n",
    "                    with open(latest_ragas, 'r') as f:\n",
    "                        ragas_data = json.load(f)\n",
    "                        method_data[\"ragas\"] = ragas_data.get(\"aggregates\", {})\n",
    "                        print(f\"  ‚úÖ RAGAS: Faith={method_data['ragas'].get('faithfulness', 0):.3f}, Relev={method_data['ragas'].get('answer_relevancy', 0):.3f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå Error reading RAGAS results: {e}\")\n",
    "            \n",
    "            comparison_data[method_name] = method_data\n",
    "    \n",
    "    if comparison_data:\n",
    "        # Save comparison summary\n",
    "        comparison_path = base_dir / \"parsing_method_comparison.json\"\n",
    "        with open(comparison_path, 'w') as f:\n",
    "            json.dump(comparison_data, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüìä Comparison summary saved to: {comparison_path}\")\n",
    "        print(\"\\nüèÜ Method Rankings:\")\n",
    "        \n",
    "        # Rank methods by key metrics\n",
    "        if len(comparison_data) > 1:\n",
    "            # LlamaIndex Hit Rate ranking\n",
    "            llama_hr_ranking = sorted(\n",
    "                [(name, data[\"llamaindex\"].get(\"hit_rate\", 0)) for name, data in comparison_data.items()],\n",
    "                key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "            print(f\"\\nüìà Hit Rate Ranking:\")\n",
    "            for i, (method, score) in enumerate(llama_hr_ranking, 1):\n",
    "                print(f\"  {i}. {method}: {score:.3f}\")\n",
    "            \n",
    "            # RAGAS Faithfulness ranking\n",
    "            ragas_faith_ranking = sorted(\n",
    "                [(name, data[\"ragas\"].get(\"faithfulness\", 0)) for name, data in comparison_data.items()],\n",
    "                key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "            print(f\"\\nüéØ Faithfulness Ranking:\")\n",
    "            for i, (method, score) in enumerate(ragas_faith_ranking, 1):\n",
    "                print(f\"  {i}. {method}: {score:.3f}\")\n",
    "    else:\n",
    "        print(\"‚ùå No evaluation data found to compare.\")\n",
    "\n",
    "# Uncomment the line below to run the comparison\n",
    "# compare_parsing_methods()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
